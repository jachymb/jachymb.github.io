---
title: 'Building a probabilistic time series model: Chapter 2 – Counting events'
layout: post
date: 2024-10-17
categories: []
tags: []
pin: true
math: true
---

In the [previous chapter](/posts/time-series-01) I stated that at the lowest level of any time series
probabilistic model are timestamps separated with randomly distributed waiting times.
However, in practice the quantity we are actually more interested in e-commerce sales
are counts of the events within a given interval.

In this chapter we study the general case when the waiting time distribution doesn't need to satisfy the memoryless
property and then show how Poisson and Gamma distribution arise assuming memorylessness of the basic events.

## Basic formalization - continuous time

Let's describe what does this mean formally. 
We still keep the i.i.d. assumption for now, but consider a general
probability distribution over $(0,\infty)$ with density $p(t)$ describing 
the probability of waiting time. The waiting times are a sequence of random variables $Y_1,Y_2,...$.
Now, the time of the $k$-th event is:

$$S_k = Y_1 + Y_2 + ... + Y_k$$

We are interested in the number of events $N(T)$ that occur in the interval $(0,T)$. That is:

$$N(T) = \max \{ k | S_k < T \}$$

For further reference, this line of thinking is developed in the [renewal theory](https://en.wikipedia.org/wiki/Renewal_theory).

## Probability of $k$ events

Consider that the $k$-th even must occur before $T$ and $(k+1)$-th event must occur after $T$.
We can write that down as an equation:

$$P(N(T) = k) = P(S_k < T \le S_{k+1}) = P(S_k < T, T \le S_{k+1})$$

Using the independence assumption we can break this into:

$$P(S_k < T, T \le S_{k+1}) = P(S_k < T) \cdot P(T \le S_{k+1})$$

The probability of the $k$-th event occuring before time $T$ is given by the cumulative distribution function (CDF)

$$F_k(T) = P(S_k < T)$$

and the probability of the $(k+1)$-th event is given by the complementary CDF. 
Putting this together, we obtain

$$\begin{equation}\label{eq:pnt}P(N(T) = k) = F_k(T) \cdot (1 - F_{k+1}(T))\end{equation}$$

This way, the problem of identifying $P(N(T)=k)$ can be reduced to identifying the cumulative density $F_k(T)$ for all $k$.

## Cumulative density of $S_k$
We could naturally ask: what is a more explicit formula for $F_k(T)$?

Obviously, for $F_1(T) = \int_0^T p(t) \mathrm{d}t$. Consider now that $S_2 = Y_1 + Y_2$ 
has an arbitrary realization where $S_2$ happens at time $t$, and $Y_1,Y_2$ happen at $y_1,y_2$ respectively.
We have $y_2 = t - y_1$. 
Since the events $Y_1, Y_2$ are assumed i.i.d., the probability density will be:

$$p_{S_2}(t) = p(y_1) \cdot p(t - y_1)$$

Summing over all possible times of $y_1$, we see that this is just convolution:

$$p_{S_2}(t) = \int_{-\infty}^{\infty} p(y_1) \cdot p(t - y_1) \mathrm{d}y_1 = (p \ast p)(t) $$

This logic can be applied inductively to obtain that $p_{S_k}(t) = p^{\ast k}(t)$ and thus:

$$F_k(T) = \int_0^T p_{S_k}(t) \mathrm{d}t = \int_0^T p^{\ast k}(t) \mathrm{d}t$$

## Special case for exponential distribution
The above approach is rather general and can be in principle used for any i.i.d. waiting times.
In the previous chapter, we have shown that the exponential distribution arises from 
a memorylessness assumption which may quite reasonable in practical modelling. 
Let's see what happens when we use this as the waiting time and plug it into the above results, that is $Y_i \sim \mathrm{Exponential}(\lambda)$.
Under this extra assumption, the renewal process is called (homogeneuous) [Poisson process](https://en.wikipedia.org/wiki/Poisson_point_process).

As a reminder, the density of the exponential distribution has a single parameter $\lambda$ and is given by:

$$p(t) = \lambda e^{-\lambda t}$$ for $t > 0$, otherwise $0$.

Convolving this density with itself produces:

$$p_{S_2}(t) = (p \ast p)(t) = \int_{0}^{t} \lambda e^{-\lambda y} \cdot \lambda e^{-\lambda (t-y)} \mathrm{d}y$$

The $y$s cancel out and we can simplify to

$$p_{S_2}(t) = \lambda^2 e^{-\lambda t} \int_{0}^{t} \mathrm{d}y = \lambda^2 e^{-\lambda t} \cdot t$$

Again, remembering that $\int_0^t t^{k-1} \mathrm{d}t = \frac{t^k}{k}$, this pattern can be inductively generalized to:

$$p_{S_k}(t) = \frac{1}{(k-1)!}\lambda^k t^{k-1} e^{-\lambda t}$$

Here, we may observe an interesting fact. This is density of a well-known distribution, the above is actually saying:

$$S_k \sim \mathrm{Erlang}(k, \lambda)$$

The [Erlang distribution](https://en.wikipedia.org/wiki/Erlang_distribution) has the CDF:

$$F_k = 1 - \sum_{i=0}^{k-1}\frac{1}{i!}\lambda^i e^{-\lambda}$$

Now by evaluating $\ref{eq:pnt}$ we get that:

$$P(N(T)=k) =  \frac{1}{k!} (\lambda T)^k e^{-\lambda T}$$

Which in this case is actually telling us that:

$$N(T) \sim \mathrm{Poisson}(\lambda T)$$

Et voilà! Going through this convoluted (pun intended) route, we eventually arrived 
at the good old familiar Poisson distribution probability mass function.

### Learning 
Using both maximum likelihood estimation
and method of moments, 
we can obtain the estimate for $\lambda$ to be the sample (arithmetic) mean:

$$\hat{\lambda}=\bar{\mathbf{n}}$$

for the counts $\mathbf{n}$.

Compare this to the reciprocal value for the exponential distribution.
The duality between the interpretation of the rate parameter for the Poisson and exponential
distributions can be seen as analogical to the duality between frequency and period.

## Special case for Gamma distribution
The [Erlang distribution](https://en.wikipedia.org/wiki/Erlang_distribution) mentioned earlier, describing the waiting time for the $k$-th memoryless event,
can be generalized to the [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution) in a straightforward way by allowing the 
shape parameter to be any positive real number, denoted usually $\alpha$ rather than $k$.

We have seen that the Erlang/Gamma distribution can be also seen as a generalization of the exponential distribution. 
(For $k=\alpha=1$ it is exponential.)
As a reminder, the Gamma distribution density is:

$$p_{\mathrm{Gamma(\alpha, \lambda)}}(t) = \frac{\lambda^{\alpha}}{\Gamma(\alpha)} t^{\alpha-1} e^{-\lambda t}$$

Even with non-integer shape parameter, the Gamma distribution can be reasonably interpreted as describing some sort of waiting time.
For example if we model the customer as shopping at Exponential-distributed intervals, but choosing _our_ shop only a fraction 
of times on average, then the waiting time at _our_ shop for this customer would be Gamma distributed with $\alpha$ reciprocal of that fraction.
Another example may be when the customer waits a bit more until they require more items to buy to save on transport costs.
In practice, we may not be sure what all the possible causes for losing the memorylessness could be,
but Gamma distribution may often be a reasonable pick to try fitting on some historical data with the extra degree of freedom.

Now, if the waiting time for a single event is Gamma, it's not too hard to show (using similar calculations as above) that 
the waiting time for the $k$-th event is:

$$ p_{\mathrm{Gamma}(\alpha, \lambda)}^{\ast k}(t) = p_{\mathrm{Gamma}(k\alpha, \lambda)}(t) $$

However, counting the events is now a bit more complicated as we now also need to account for the $\alpha$.
The Gamma CDF plugged into $\ref{eq:pnt}$ cannot be manipulated into a reasonable form using its basic properties and elementary algebra.

However, there is a theorem in renewal theory,
which says that the [probability-generating function](https://en.wikipedia.org/wiki/Probability-generating_function)
of the counts is given by:

$$\begin{equation}\label{eq:pgf}G_{N(T)}(z) = \mathcal{L}^{-1}\left(\frac{1-\phi(r)}{r(1-z\phi(r))}\right)(T),\end{equation}$$

where $\mathcal{L}^{-1}$ denotes the inverse Laplace transform 
and $\phi(r) = \mathrm{E}[e^{-rY_i}] = \int_{-\infty}^{\infty} e^{-rt} p_{Y_i}(t)\mathrm{d}t$ is the Laplace transform of the waiting time PDF. 

Knowing that in this case when $Y_i \sim \mathrm{Gamma}(\alpha,\lambda)$ we can easily look up or calculate that here:

$$\phi(r) = (1+r\lambda)^{-\alpha}$$

plugging this into the right hand side of $\eqref{eq:pgf}$ and some algebra tells us we need to Laplace invert 

$$\left(r-\frac{r(z-1)}{(1+\lambda r)^{\alpha}-1}\right)^{-1}$$

## Other continuous distributions
If the memoryless assumption doesn't seem realistic, it can sometimes be worked around using the general method
(either directly from $\ref{eq:pnt}$ or using the Laplace transform method outlined in the Gamma waiting time special case).
But even that is not always solvable analytically, and we would need to resort to numerical methods.
For example when we use the Weibull distribution, instead of exponential,
the convolution cannot be expressed in a closed form anymore .

Thus, when using the exponential distribution model, we should either refer to the memoryless assumption or simply admit
that it is a computational convenience.

## Discrete time
We have seen in the previous chapter that for discrete time, 
the memoryless assumption implies the individual events have i.i.d. distribution $\mathrm{Bernoulli}(\lambda)$.
If follows easily, that the number of events that happen within an interval of length $T$ 
has the $\mathrm{Binomial}(T, \lambda)$.

When I was first taught about Poisson distribution, I was told it can be seen as a limit case of the Binomial distribution
in the sense that when keep $T \cdot \theta = \lambda$ fixed then for any $k \in \mathbb{N}$:

$$\lim_{(T,\theta)\to(\infty,0)} p_{\mathrm{Binomial}(T,\theta)}(k) = p_{\mathrm{Poisson}(\lambda)}(k) $$

In the context of using these two distribution to describe counts of events, this intuitively makes sense:
With discrete time model, with the granularity getting finer (time becoming more continuous),
$T$ grows for the same amount of physical time (more time units per interval of fixed duration)
and $\theta$ decreases (lesser probability of event occurring within a shorter window).

That also means we can use the simpler Poisson distribution as a good approximation when $\theta$ is low and $T$ is high,
for example when we want to count events per day and an event may be recorded at each second.

At this point, we could also ask:
If the continuous time for waiting for a memoryless event is exponential and the waiting time for $k$-th such event is Gamma,
then if the discrete time for waiting for a memoryless event is geometric, what is the waiting time for $k$-th such event?
The answer is the [Negative binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution).
This could again be derived analogically as the self-convolution of geometric distribution.

We can now summarize these analogies into the following table:

| Time model                      | Waiting time for next event<br>$Y_i$ | Waiting time for $k$-th event<br>$S_k$ | Count of events<br>$N(T)$                      |
|---------------------------------|--------------------------------------|----------------------------------------|------------------------------------------------|
| Continuous $T \in \mathbb{R}^+$ | $\mathrm{Exponential}(\lambda)$      | $\mathrm{Erlang}(k, \lambda)$          | $\mathrm{Poisson}(T \lambda)$                  |
| Continuous $T \in \mathbb{R}^+$ | $\mathrm{Gamma}(\alpha, \lambda)$    | $\mathrm{Gamma}(k\alpha, \lambda)$     | $\mathrm{NegBin}(\alpha, \frac{T}{T+\lambda})$ |
| Discrete $T \in \mathbb{N}$     | $\mathrm{Geometric}(\lambda)$        | $\mathrm{NegBin}(k, \lambda)$          | $\mathrm{Bin}(T, \lambda)$                     | 


### Takeaway and notes
We derived the Poisson distribution as the count of events of a random process from nothing but the following assumptions:
* The events are i.i.d.
* The waiting time between the events is a memoryless continuous random distribution.

We can also see how the Poisson distribution is intimately related to the Gamma distribution:
The Poisson gives counts within an interval and the Gamma gives the length of the interval to achieve a given count,
both using a common rate parameter.
This close relation is further emphasised in Bayesian statistics where the Gamma distribution can serve at the conjugate prior for the rate
of the exponential, Poisson or another Gamma distribution.

