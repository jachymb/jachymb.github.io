---
title: 'Building a probabilistic time series model: Chapter 1 – Waiting times'
layout: post
date: 2024-10-16
categories: []
tags: []
pin: true
math: true
---

In this chapter, I offer some answers to the question:
_How do we model the waiting time to the next event?_
But I first discuss how does this fit into the bigger picture and goals outlined in the [intro](/posts/time-series-00).
If you are only interested in the math, you may [skip the discussion](#simplest-model).

# Discussion on data granularity

In the [intro](/posts/time-series-00),
we defined the dataset of past events to be a relation over $\mathrm{SKUs} \times \mathrm{Timestamps} \times \mathbb{N}$.

In data science, relations are normally represented by database tables, so we have a table like this:

| SKU              | Timestamp | Amount   |
|------------------|-----------|----------|
| $\mathrm{SKU_1}$ | $t_1$     | $n_1$    |
| $\mathrm{SKU_2}$ | $t_2$     | $n_2$    |
| $\mathrm{SKU_3}$ | $t_3$     | $n_3$    |
| $\vdots$         | $\vdots$  | $\vdots$ |
| $\mathrm{SKU_N}$ | $t_N$     | $n_N$    |

Now, timestamps may be stored with varying amounts of granularity.
When the timestamp is saved with seconds precision,
it can be considered a fine granularity for e-commerce purposes and be represented mathematically by a real number.
When it's with a coarse granularity, such as daily, it's probably better seen as natural number.

With real-number representation, we don't have to worry much about
two events happening at the same time since this almost never happens.
But with increasingly coarse granularity, this needs to be considered
that either 

1. the Amount column contains pre-summed data of individual event amounts that occurred within the same time window, or
2. the table is actually some multi-relation, rather than a relation (i.e. in database terms, the (SKU, Timestamp) pair doesn't for a unique key.)

However, in the [next chapter](/posts/time-series-02), we will study, how the distributions of counts of events for the coarser granularity arise from that.

# Breaking the problem down
Ideally, it'd be cool to have a full joint distribution over these three things.

But in e-commerce practice, we can make the following simplifications:
  * The SKU is non-random. We don't care which SKU is purchased the next or questions like this.
    We want to know, given a SKU and given a (future) time window, how many products will be demanded.
  * I claim that the timestamps and amounts are actually reasonably modelled as independent.
    In reality, they may be correlated in time to some extent,
    but it can be argued that the frequency of how often customers make a purchase doesn't influence how many pieces they order very much.
    In a specific use case, the validity of this assumption should be examined and criticized, but I go with it as it provides a great deal of mathematical convenience:
    It allows to study the Timestamps and Amounts separately.
    If this is considered to be unrealistic even as an approximation, we would need to model the join distribution of timestamps and amounts.

Having considered this, in this chapter we further focus only on the timestamps.
As a subproblem, we are now asking only the following question: 
_How do we model the waiting time to the next event at each moment in time?_

# <span id="simplest-model">Waiting times – the simplest model – continuous time</span>
In this section, we will model time as continuum (i.e. timestamps are represented by real numbers)
and we make two assumptions that could be considered rather strong in the time series context:
1. The waiting times are [i.i.d](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). 
   In the time series case, this means the distribution parametrization  is constant in time.

   This may seem overly naïve in many cases (e.g. because of popularity trends or seasonalities), 
   but keep in mind that it will  hold at least locally
   in most practical cases: there will be some sort of continuity to the distribution change (it's locally constant).

   Violations to this assumption are modelled in [Chapter 3](/posts/time-series-03).   
2. The distribution is [memoryless](https://en.wikipedia.org/wiki/Memorylessness).

    This means, the time already spent waiting for an event does not affect how much longer we need to wait for the next one to occur.
    More formally, this is written as 

    $$P(Y > t+s | Y \ge t)=P(Y > s)$$

    for the random variable $Y$ describing the waiting time between events, where $t\ge0$ can be interpreted as time already waited and $s\ge0$ as time yet to wait.
    
    Let's consider when this is justifiable. A customer not caring about how long ago the previous customer purchased something seems like a reasonable assumption. On the other hand, if we sell a consumable product, and the same customer wants to buy it again soon but not immediately, the memoryless assumption may no longer be appropriate.
 

It can be shown rigorously ([proof on Wikipedia](https://en.wikipedia.org/wiki/Memorylessness)), that the continuous memomoryless distribution
needs to have density 

$$p(t|\lambda) \propto e^{-\lambda t} $$

for some parameter $\lambda \in (0, \infty)$.
This is of course the [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution).

The idea of the proof is to rewrite (using conditional probability definition) the memorylessness assumption as

$$P(Y > t+s) = P(Y > s) \cdot P(Y > t)$$

and notice that this describes a situation where the complementary cumulative distribution function maps addition to multiplication,
which only an exponential function can do.

## Learning 
Using both [maximum likelihood estimation](https://www.statlect.com/fundamentals-of-statistics/exponential-distribution-maximum-likelihood)
and [method of moments](https://math.stackexchange.com/questions/2943668/method-of-moments-exponential-distribution),
we can obtain the estimate for $\lambda$ to be the reciprocal of the sample (arithmetic) mean:

$$\hat{\lambda}=\frac{1}{\bar{\mathbf{y}}}$$

for the waiting times $\mathbf{y}$.
This aligns nicely the fact that $\mathrm{E}\[Y\] = \frac{1}{\lambda}$.

## Assumptions remarks
Note that only the memoryless assumption and time continuity is needed to derive the expnential distribution, nothing else.
The i.i.d. assumption lives on a conceptually higher level, but I wanted to emphasise the fact that
we will eventually be modelling time series and the i.i.d. assumption would imply the $\lambda$ parameter here is constant in time.
But of course, we need the i.i.d. to reasonably learn the parameter from data.

If we consider the memorylessness assumption to be unrealistic, 
simple distributions often used to describe waiting times include [Weibull](https://en.wikipedia.org/wiki/Weibull_distribution), [Lomax](https://en.wikipedia.org/wiki/Lomax_distribution)
and [Gamma](https://en.wikipedia.org/wiki/Gamma_distribution) but in principle any distribution with $(0,\infty)$ support can be used – when justified.
These both generalize the exponential distribution in different directions. More on that in later chapters.

# Memoryless waiting times – discrete time
Consider we have discrete time instead.
The discrete variant for the memorylessness assumption can formally be written the same but now $t,s \in \mathbb{N}$.

In this case, it can be shown that the individual events need to be i.i.d. Bernoulli distributed.
Intuitively, this makes sense: At each time, the event either happens or not, but with no memory, it needs to be independent Bernoulli trial.
But since we assumed the rate is constant, they are also identical.
As a consequence, the distribution of the time units to wait before the event occurs is [Geometric](https://en.wikipedia.org/wiki/Geometric_distribution)
which has probability mass function:

$$ p(t|\lambda) \propto  e^{-\lambda t}$$

Using a somewhat unconventional parametrization with $\lambda \in (0, \infty)$ to show that it really is the discrete variant of the exponential distribution.

# Honorable mention - Weibull distribution
The Weibull distribution is a simple non-memoryless distribution used to model waiting times.
Again, to show similary with the exponential distribution, the density of the Weibull distribution can be written as:

$$ p(t|\lambda,\alpha) \propto e^{(\alpha-1)\ln(\lambda t) - (\lambda t)^{\alpha} } $$

where both $\lambda,\alpha \in (0,\infty)$. From here it's immediately obvious that it reduces to exponential distribution for <nobr>$\alpha=1$.</nobr>
With $\alpha>1$ the probability we will wait a long time decreases with the time we have already spent waiting.
This may for example be useful for modelling consumable goods that customers will buy repeatedly:
The longer we wait, the bigger the change the product has been consumed by the customer and thus want a new one.
Whether adding one more parameter this way is sufficient to faithfully capture such behavior should be studied with an exploratory analysis,
but it has proven useful in many practical cases.

# STAN demo – fitting exponential distribution

```stan
data {
  int<lower=1> N;
  array[N] real<lower=0> y;  // Waiting times
}
parameters {
  real<lower=0> lambda;
}
model {
  y ~ exponential(lambda);
}
```
