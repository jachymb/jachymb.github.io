---
title: 'From linear regression to GLMs – theory and examples'
layout: post
date: 2024-12-07
tags: [statistics,time series,regression,case study,probabilistic modelling]
pin: true
math: true
excerpt_separator: '<!--more-->'
---

In this article, I dig a bit under the surface of the theory around good'ol linear regression
and explain how it can be extended to generalized linear models and why GLMs may be useful on some examples.

<!--more-->

One important case where linear regression fails when modelling time series is when the data are positive integers close to zero
(e.g. daily counts of purchases of a low-selling product.) 
This is because there are certain assumptions encoded in the model, 
that we often justify with some handwavy reference
to the central limit theorem.
But in this case it may easily lead to poor results: 
For small counts, the distinction between normal and a discrete counting distribution will be significant.
(See the article about [counting events](/posts/counting-events) for a more detailed discussion.)

Generalized linear models (GLMs) max fix this and similar issues by allowing other kinds of data distributions 
while keeping a lot of the desirable and familiar properties of linear regression.

First, let's study the relation of linear regression and ordinary least squares (OLS). 

## Gauss–Markov theorem
OLS are the most basic and most common method to make a linear fit
and is usually justified by referring to the [Gauss–Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem)
which says that ordinary least squares are the best linear unbiased estimator under certain conditions. 

In particular:
We call an estimator unbiased when $\mathrm{E}[\hat{\beta}] = \beta$. 
We say it's best in the sense of [statistical efficiency](https://en.wikipedia.org/wiki/Efficiency_(statistics)),
i.e. we want to minimize $\mathrm{Var}[\hat{\beta}]$.

Have 

$$\begin{equation}\label{eq:linreg}y = X\beta + \varepsilon,\end{equation}$$

where $y,\varepsilon \in \mathbb{R}^n, \beta \in \mathbb{R}^m, X \in \mathbb{R}^{n\times m}$
and individual $\varepsilon_i$ are understood as random variables.
Since $y$ depends on $\varepsilon$, can also be analyzed as a random variable and so can be any estimate $\hat{\beta}$ which depends on $y$.
Assume the error terms $\varepsilon$ have zero mean, i.e. $\mathrm{E}[\varepsilon_i] = 0$,
and all have the same variance (homoscedasticity) and are uncorrelated i.e. $\mathrm{Cov}[e_i, e_j] = \sigma^2 \delta_{ij}$ where $\sigma \in \mathbb{R}^+$ 
(here using the Kronecker delta symbol).
Gauss-Markov theorem says that under these conditions,
the OLS estimator, given by 

$$\hat{\beta} = \arg\min_{\beta} \lVert X\beta - y \rVert_2^2 = \arg\min_{\beta} \sum_i | X_i \beta - y_i|^2 = \left(X^T X\right)^{-1} X^T y$$

is the best estimator among those which are unbiased.

This is quite powerful, because it doesn't specify which distribution $\varepsilon$ has, only that it needs to have certain properties.

But what if we decided we want to maximize efficiency, perhaps even at the cost of having the estimator biased? 

## MLE for linear regression

To do MLE, we need a distribution.
So let's now specify what distribution the error terms $\varepsilon$ have.
The most common choice of model is:

$$\begin{equation}\label{eq:normalerr} \varepsilon \sim \mathrm{Normal}(0, \; \sigma^2), \end{equation}$$

where $\sigma \in \mathbb{R}^+$ is constant.
(_Note_: I am using the vectorized notation of random variables. The distribution is univariate, and the above actually means each $\varepsilon_i$ has the distribution independently.)
The assumptions of the Gauss–Markov theorem are satisfied.
Coincidentally, it <span title="Cool exercise for univariate case">can be shown</span>
that the maximum likelihood estimation of $\beta$ is in this case again OLS.
This may seem unsurprising, but let's have a look at another example.

What if the errors have a different distribution? For example [Laplace](https://en.wikipedia.org/wiki/Laplace_distribution) (a.k.a. double exponential).

$$ \varepsilon \sim \mathrm{Laplace}\left(0, \; \theta'\right)$$

Now the assumptions of the Gauss–Markov theorem are still satisfied, but the MLE is the least absolute deviations (LAD),
in other words, we use the $l^1$ norm instead of $l^2$:

$$\hat{\beta} = \arg\min_{\beta}\ell(X,y\,|\,\beta) = \arg\min_{\beta} \lVert X\beta - y \rVert_1 = \arg\min_{\beta} \sum_i | X_i \beta - y_i|$$

Under usual circumstances, [MLE estimates are statistically efficient](https://gregorygundersen.com/blog/2019/11/28/asymptotic-normality-mle/), at least asymptotically.
The following plot shows some synthetic data with Laplace distributed noise around the red line.
The fit obtained from the (biased) MLE is in this case closer to the true generating line on most of the interval,
than the unbiased OLS suggested by Gauss–Markov.
To understand how often this happens, we'd need to analyze the bias–efficiency relation exactly in more detail, but I'm 
not doing that now. 
In general though, the difference is not very big, so in practical cases, consider whether this is worth the headache. 

![](/assets/images/linreg_ols_vs_mle.png "OLS vs MLE for Laplace distributed errors")

(_Fun fact:_ If you want general $l^p$ norm, there is [Generalized normal distribution](https://en.wikipedia.org/wiki/Generalized_normal_distribution).
The limit case for $p \to \infty$ with the max norm corresponds to uniform distribution of errors.)

The takeaway here should be that if we aren't satisfied with the handwavy reference to CLT, 
and we have a reason to believe the distribution of the data is other than normal, 
we can likely do better than OLS.
Note that the Laplace distribution has heavier tail than normal, 
so LAD may be practical to use in the presence of outliers in the data as it's more robust.

## Generalized linear models
With the Laplace distribution instead of normal, we have touched the idea of using different noise distribution.
[Generalized linear models](https://en.wikipedia.org/wiki/Generalized_linear_model) (GLM) take this one step further. 
First, consider the case with the normal distributed noise and make a few simple observations.
Putting together $\eqref{eq:linreg}$ and $\eqref{eq:normalerr}$ can be written as:

$$\begin{equation}\label{eq:normalerr2} y \sim \mathrm{Normal}\left(X\beta, \; \sigma^2 \right) \end{equation}$$

and thus also trivially:

$$ \mathrm{E}\left[y\,|\,X\right] = X\beta.$$

Now we want to generalize this to another distribution $\mathcal{D}\left(\theta, \theta'\right), \theta \in \Theta$.
Basically we want to plug $X\beta$ into the distributions parameter which represents the mean value. 
A lot of distributions don't have that and may not even allow negative values though,
so we allow a reparametrizazion function $g: \Theta \to \mathbb{R}$ (invertible), called the _link function_ and require:

$$\begin{equation}\label{eq:link1}y \sim \mathcal{D}\left(g^{-1}\left(X\beta\right), \; \theta'\right) \end{equation}$$

and

$$\begin{equation}\label{eq:link2} \mathrm{E}\left[y\,|\,X\right] = g^{-1}(X\beta). \end{equation}$$

(Don't ask me why the inverse link is used in most places, it's just a weird convention.) 
The choice of the link function is kind of arbitrary as long as its domain is the parameter space,
but it does have an effect on how the model performs and how it should be interpreted.
The treatment of the extra parameters $\theta'$ may vary. 
In the simplest cases they don't exist or are set to a fixed known value.
Otherwise, they may be treated as nuisance parameters and approached with profile likelihood (or marginalization in a Bayesian setting)
or they may be fully fledged parameters estimated jointly with $\beta$, which then may make the estimation more difficult.
These details may be implementation–specific in software.

When the distribution $\mathcal{D}$ is from the [exponential family](https://en.wikipedia.org/wiki/Exponential_family)
and $\eqref{eq:link1},\eqref{eq:link2}$ hold, there are available good-performance algorithms to compute the likelihood and to maximize it. 
which are implemented in numerous libraries.

A famous instance of a GLM is the logistic regression, where $\mathcal{D}(\theta), \theta \in (0,1)$ is the Bernoulli distribution, 
and we use the logit function as the link. So in terms of $\eqref{eq:link1}$,
logistic regression can be characterized as the probabilistic model:

$$ y \sim \mathrm{Bernoulli}\left(\frac{1}{1+e^{-X\beta}}\right) $$

When $y$ is binary, the Bernoulli distribution is the only option, but as I mentioned, 
we can choose a different link function. 
In this case, quantile function of normal distribution is another possible option which
is also sometimes used as the link function, $y \sim \mathrm{Bernoulli}(\Phi(X\beta))$ leading to the somewhat less popular [probit model](https://en.wikipedia.org/wiki/Probit_model).
Similarly, we could construct a link function for Bernoulli distribution by properly shifting and scaling $\tan$ or $\tanh^{-1}$.

As a side note, relating to machine learning, this GLM structure also reveals quite clearly, 
how any Bernoulli GLM is a perceptron generalization,
which can provide probabilities on top of classification. 
Btw, this means, the output layer of any single-class ANN classifier is just a Bernoulli GLM taking the previous layer as features. 

For discrete counts data $y$, we can use distributions like Poisson, binomial, geometric or negative binomial
which are all in the exponential family and are thus GLM–compatible. 

## GLM - example with data  
The following plot shows a comparison of using the OLS (a.k.a. Normal-Identity GLM) versus Poisson GLM with log link
on some counts of daily purchases data. Just to be super clear, the models I'm comparing here are $\eqref{eq:normalerr2}$ and:

$$ y \sim \mathrm{Poisson}\left(e^{X\beta}\right) $$

Although it may not be obvious at the first sight, 
the data do have some sort of trend growth and weekly periodicity,
in particular there are less sales on weekends which is something modelling here.

For both GLMs, there are the same three features: intercept, the time (horizontal axis),
and a boolean indicator for weekends. 

![](/assets/images/normal_vs_poisson_glm.png "Normal vs Poisson GLM")


A glaring (qualitative) issue with the OLS is that it can (and does near the axes origin) go 
into negative numbers, which would need to be solved using some ad-hoc technique like clipping which feels pretty arbitrary.
The Poisson-log model will always produce a positive expectation value.
But it has its own (quantitative) issue, here it's simplicity is at the cost of an exponential trend growth
which is unrealistic to use for extrapolation beyond very short term.
(This can be avoided by not using time as a feature directly, but when we do, the log link must be interpreted as saying "we're expecting exponential growth").
In this example, the Poisson-log model has better quality metrics like BIC, RMSE, MAE, but only marginally. 

Where the qualitative difference between the models becomes more obvious is when we look at the prediction intervals for $y$.
The following plot shows the prediction intervals in which 90% of the most likely outcomes will be
(exactly 90% for OLS, at least 90% for Poisson GLM due to its discrete nature, inclusive on the boundary.)
Quantitatively, the bounds for the Poisson GLM are mostly tighter (despite covering greater probability), which is desirable.

![](/assets/images/normal_vs_poisson_glm_intervals.png "Normal vs Poisson GLM")

Suppose a business customer asks you, how many items is he going to sell tomorrow.
Do you think he better likes the answer "between 0 and 3" or "between minus 1.9 and 4.33"?

## How are the intervals computed?
This is something that was a bit skipped over in my statistical data analysis class at university,
so I needed to learn that later on. 
Again I want to explain how this works for vanilla linear regression $\eqref{eq:normalerr2}$ and then, how that extends to GLMs.
There are two intervals (or bands of their boundaries) that we usually may be interested in:
The confidence interval of the fit and the wider prediction interval, illustrated on the following plot which fits an OLS
on some data with normally distributed error.

![](/assets/images/linreg_bands.png "Confidence bands on OLS fitted on data with normally distributed errors")


Since $\hat{\beta}$ is a random variable, the expectation of the model $X \hat{\beta}$ is also random.
Denoting the data covariance matrix $\Sigma=X^T X$ and using the fact that MLE is $\hat{\beta} = \Sigma^{-1} X^T y$ together with a little bit of algebra exercise, 
we can obtain:

$$\begin{equation}\label{eq:beta_estimate} \hat{\beta} \sim \mathrm{MultivariateNormal}\left(\beta, \; \sigma^2 \Sigma^{-1}\right) \end{equation}$$

Now the confidence interval of the fit asks "How far is $X \hat{\beta}$ from $X \beta$ within a given confidence level?"
On the plot, this is the inner region in which the red line falls with 90% certainty. 
Knowing the distribution of $\hat{\beta}$ makes this straightforward to compute using standard techniques.
The exact formula depends on whether we consider $\sigma$ constant or the choice of estimation of $\hat{\sigma}$.
See also: [standard error](https://en.wikipedia.org/wiki/Standard_error). 
But I find the prediction interval more interesting, so let's focus on that.

The prediction interval asks "What is the range of a future observed value $\hat{y} = y\,|\,X,x$ with a given confidence level?"
Here $X$ is the past observed data matrix and $x \in \mathbb{R}^m$ is a new vector of features for which we're making a prediction. 
To answer this we will identify the distribution of $\hat{y} \in \mathbb{R}$. In this case, we have:

$$y\,|\,x,\hat{\beta},\hat{\sigma} \sim \mathrm{Normal}(x^T\hat{\beta},\; \hat{\sigma}^2)$$

where we need to additionally specify that we use MLE to estimate $\hat{\sigma}^2$,
which is given by the usual population variance formula:

$$\hat{\sigma}^2 = \frac{1}{n}\|y-X\hat{\beta}\|_2^2$$

and has the $\chi^2$ distribution variant:

$$\begin{equation}\label{eq:sigma_estimate} \hat{\sigma}^2 \sim \mathrm{Scaled–Inverse–}\chi^2(n, \; \sigma^2) \end{equation}$$

(_Side note:_ Here $\hat{\sigma}$ is biased, but it's more efficient than the unbiased formula with $n-m$ in denominator. This again illustrates that MLE prefers efficient estimates over unbiased.
The choice of estimate affects the subsequent results.)
Anyway, to obtain the distribution of $\hat{y}$ we need to marginalize out both random variables $\hat{\beta},\hat{\sigma}$. 
This is the same as the predictive posterior distribution in Bayesian statistics where we interpret $\eqref{eq:beta_estimate},\eqref{eq:sigma_estimate}$ as priors.
It is a bit more tedious calculation, but it can be done somewhat routinely to obtain that $\hat{y}$ has the [t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) (shifted and variance-scaled):

$$\begin{equation}\label{eq:y_predictive} \hat{y} \sim t_{n} \left(x^T\beta, \; \sigma^2 \left(1 + x^T\Sigma^{-1} x\right)\right) \end{equation}$$

So when we want precise prediction bands, we can just use this. However, it may be a bit awkward to work with,
and it doesn't generalize well to other cases. 
So asymptotics to the rescue!
With growing number $n$ of samples in  $X \in \mathbb{R}^{n\times m}$, considering that for the inverse data covariance matrix
$\lim_{n\to\infty} \Sigma^{-1} = \mathbf{0}$
we are kinda back where we started and get
back to the original normal distribution in the sense that:

$$ \lim_{n\to\infty}\hat{y} \sim \mathrm{Normal}(x^T \beta, \; \sigma^2) $$

or we could basically just write:

$$ \hat{y} \approx y $$

This result should be intuitive:
When we have enough data, there is little uncertainty about the coefficients $\hat{\beta}$ and thus this uncertainty doesn't affect the prediction interval much; only the natural variance of $y$ does.
It is also why in the plot, the prediction interval seems flat, even though it isn't exactly (since the covariance in $\eqref{eq:y_predictive}$ depends on the position on the horizontal axis). 

In general, for GAMs, there isn't an explicit formula like $\eqref{eq:y_predictive}$ but the asymptotics 
are analogous and we get:

$$\lim_{n\to\infty} \hat{y} \sim \mathcal{D}\left(g^{-1}\left(x^T\beta\right), \; \theta'\right)$$


So for a confidence level $\gamma \in (0, 1)$, the approximate prediction interval is
<span title="in terms of Lebesgue measure">the smallest set</span> $C$
under the constraint: 

$$\int_C p_{\mathcal{D}}\left(y\,|\,g^{-1}\left(x^T\beta\right),\; \theta'\right) \, \mathrm{d}y \ge \gamma$$

or analogically a sum for discrete distribution.
Obviously, in practice, we must substitute the estimated values in place of $\beta, \theta$.
This is how I constructed the prediction intervals for the Poisson–log GLM.
Note that for multimodal $\mathcal{D}$, the set $C$ may not be an interval, but rather a disjoint set of intervals,
but idk about any case where that would happen in modelling practice.

This asymptotic approach to confidence bands is problematic for long-term extrapolation though, even with lots of data. 
If we don't want to use the asymptotic approximation, we must often resort to numerical methods.  
A cool trick is to set some (non-informative) Bayesian priors over $\beta,\theta'$ and use MCMC style algorithms to compute 
posteriors of $\hat{y},\hat{\beta},\hat{\theta'}$ which can serve to analyze their uncertainty and determine the desired intervals.
