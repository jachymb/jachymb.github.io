---
title: 'Building a probabilistic time series model from scratch: Chapter 0 – Waiting times'
layout: post
date: 2024-10-16
categories: []
tags: []
pin: true
math: true
---

Eventually, in this series, we will build a rather elaborate hierarchical predictive time-series model. The motivating application in this series is modeling e-commerce sales. However, I don't just want to present the final result. Rather, let's explore the entire thought process that leads us there, starting from the very basics.

On the lowest level of data available for time series modeling, we have timestamps of events, possibly with some annotations (e.g., type of product, amount). In practice, we often work with pre-aggregated data, such as total daily sales. However, if we want to be explicit about all the assumptions we are making, we should consider the individual events as the elementary units.
In this chapter, I discuss how the exponential distribution arises from a basic assumption.

## The core questions

The core questions we are asking is: _What is the distribution of total amount within a given time interval?_

Using the scenario with per-event amount, we can break this down into:
1. _What is the probability distribution of the waiting time to the next event at each moment in time?_
2. _What is the probability distribution of the amount of the next event?_

Any time series model is basically an attempt to answer this,
each of them using a different set of assumptions, simplifications and approximations.

For now, we will only focus on answering question number 1 as it seems more challenging in practice.
In e-commerce the rate of orders may reasonably be expected to change much more than the amount requested in each single order.

## Waiting times – the simplest model

We will model time as continuum (i.e. timestamps are represented by real numbers)
and we make two assumptions that could be considered rather strong in the time series context:
1. The waiting times are [i.i.d](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). 
   In the time series case, this means the distribution parametrization  is constant in time.

   This may seem overly naïve in many cases (e.g. because of popularity trends or seasonalities), 
   but keep in mind that it will  hold at least locally
   in most practical cases in the sense that while the distribution does in fact change in time, there will be some sort of continuity to it.
2. The distribution is [memoryless](https://en.wikipedia.org/wiki/Memorylessness).

    This means, the time already spent waiting for an event does not affect how much longer we need to wait for the next one to occur.
    More formally, this is written as 

    $$P(Y > t+s | Y > t)=P(Y > s)$$

    for the random variable $Y$ describing the waiting time between events, where $t>0$ can be interpreted as time already waited and $s>0$ as time yet to wait.
    
    Let's consider when this is justifiable. A customer not caring about how long ago the previous customer purchased something seems like a reasonable assumption. On the other hand, if we sell a consumable product, and the same customer wants to buy it again soon but not immediately, the memoryless assumption may no longer be appropriate.
 

It can be shown rigorously, that the continuous memomoryless distribution
needs to have density 

$$p(t) \propto \exp(-\lambda t)$$

for some parameter $\lambda>0$.
This is of course the [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution).

The idea of the proof is to rewrite (using conditional probability definition) the memorylessness assumption as

$$P(Y > t+s) = P(Y > s) \cdot P(Y > t)$$

and notice that this describes a situation where the complementary cumulative distribution function maps addition to multiplication,
which only an exponential function can do.

## Learning 
Using both [maximum likelihood estimation](https://www.statlect.com/fundamentals-of-statistics/exponential-distribution-maximum-likelihood)
and [method of moments](https://math.stackexchange.com/questions/2943668/method-of-moments-exponential-distribution),
we can obtain the estimate for $\lambda$ to be the reciprocal of the sample (arithmetic) mean:

$$\hat{\lambda}=\frac{1}{\bar{\mathbf{y}}}$$

for the waiting times $\mathbf{y}$.
This aligns nicely the fact that $\mathrm{E}\[Y\] = \frac{1}{\lambda}$.


## Assumptions remarks
Note that only the memoryless assumption is needed to derive the expnential distribution.
The i.i.d. assumption lives on a higher level, but I wanted to emphasise the fact that
we will eventually be modelling time series and the i.i.d. assumption would imply the $\lambda$ parameter is constant in time.
But of course, we need the i.i.d. to reasonably learn the parameter from data.

If we consider the memorylessness assumption to be unrealistic, 
simple distributions often used to describe waiting times include [Weibull](https://en.wikipedia.org/wiki/Weibull_distribution)
and [Gamma](https://en.wikipedia.org/wiki/Gamma_distribution) but in principle any distribution with $(0,\infty)$ support can be used when justified.
These both generalize the exponential distribution in different directions. More on that in later chapters.

However, the identical distribution part of the i.i.d. assumption seems way more problematic if we want to model anything non-trivial. 
We focus on this in the subsequent chapters.


## STAN demo – fitting exponential distribution

```stan
data {
  int<lower=1> N;
  array[N] real<lower=0> y;  // Waiting times
}
parameters {
  real<lower=0> lambda;
}
model {
  y ~ exponential(lambda);
}
```
