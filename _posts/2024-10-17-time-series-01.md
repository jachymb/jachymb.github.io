---
title: 'Building a probabilistic time series model from scratch: Chapter 1 – Counting events'
layout: post
date: 2024-10-17
categories: []
tags: []
pin: true
math: true
---

In the previous chapter I claimed that at the lowest level of any time series
probabilistic model are timestamps separated with randomly distributed waiting times.
However, in practice the quantity we are actually more interested in e-commerce sales
are counts of the events within a given interval.

In this chapter we study the general case when the waiting time distribution does not need to satisfy the memoryless
property and then show how Poisson and Gamma distribution arise assuming memorylessness of the basic events.

## Basic formalization

Let's describe what does this mean formally. 
We still keep the i.i.d. assumption for now, but consider a general
probability distribution over $(0,\infty)$ with density $p(t)$ describing 
the probability of waiting time. The waiting times are a sequence of random variables $Y_1,Y_2,...$.
Now, the time of the $k$-th event is:

$$S_k = Y_1 + Y_2 + ... + Y_k$$

We are interested in the number of events $N(T)$ that occur in the interval $(0,T)$. That is:

$$N(T) = \max \{ k | S_k < T \}$$

For further reference, this line of thinking is developed in the [renewal theory](https://en.wikipedia.org/wiki/Renewal_theory).

## Probability of $k$ events

Consider that the $k$-th even must occur before $T$ and $(k+1)$-the event must occur after $T$.
We can write that down as an equation:

$$P(N(T) = k) = P(S_k < T \le S_{k+1}) = P(S_k < T, T \le S_{k+1})$$

Using the independence assumption we can break this into:

$$P(S_k < T, T \le S_{k+1}) = P(S_k < T) \cdot P(T \le S_{k+1})$$

The probability of the $k$-th event occuring before time $T$ is given by the cumulative distribution function (CDF)

$$F_k(T) = P(S_k < T)$$

and the probability of the $(k+1)$-th event is given by the complementary CDF. 
Putting this together, we obtain

$$P(N(T) = k) = F_k(T) \cdot (1 - F_{k+1}(T)).$$

## Cumulative density of $S_k$
We could naturally ask: what is a more explicit formula for $F_k(T)$?

Obviously, for $F_1(T) = \int_0^T p(t) \mathrm{d}t$. Consider now that $S_2 = Y_1 + Y_2$ 
has an arbitrary realization where $S_2$ happens at time $t$, and $Y_1,Y_2$ happen at $y_1,y_2$ respectively.
We have $y_2 = t - y_1$. 
Since the events $Y_1, Y_2$ are assumed i.i.d., the probability density will be:

$$p_{S_2}(t) = p(y_1) \cdot p(t - y_1)$$

Summing over all possible times of $y_1$, we see that this is just convolution:

$$p_{S_2}(t) = \int_{-\infty}^{\infty} p(y_1) \cdot p(t - y_1) \mathrm{d}y_1 = (p \ast p)(t)$$

This logic can be applied inductively to obtain that $p_{S_k}(t) = p^{\ast k}(t)$ and thus:

$$F_k(T) = \int_0^T p_{S_k}(t) \mathrm{d}t = \int_0^T p^{\ast k}(t) \mathrm{d}t$$

## Special case for exponential distribution
The above approach is rather general and can be in principle used for any i.i.d. waiting times.
In the previous chapter, we have shown that the exponential distribution arises from 
a memorylessness assumption which may quite reasonable in practical modelling. 
Let's see what happens when we use this as the waiting time and plug it into the above results.
Under this extra assumption, the renewal process is called (homogeneuous) [Poisson process](https://en.wikipedia.org/wiki/Poisson_point_process).

As a reminder, the density of the exponential distribution has a single parameter $\lambda$ and is given by:

$$p(t) = \lambda e^{-\lambda t}$$ for $t > 0$, otherwise $0$.

Convolving this density with itself produces:

$$p_{S_2}(t) = (p \ast p)(t) = \int_{0}^{t} \lambda e^{-\lambda y} \cdot \lambda e^{-\lambda (t-y)} \mathrm{d}y$$

The $y$s cancel out and we can simplify to

$$p_{S_2}(t) = \lambda^2 e^{-\lambda t} \int_{0}^{t} \mathrm{d}y = \lambda^2 e^{-\lambda t} \cdot t$$

Again, this pattern can be inductively generalized to:

$$p_{S_k}(t) \propto \lambda^k t^{k-1} e^{-\lambda t}$$

As a side note, we may observe an interesting fact. The above is actually saying:

$$S_k \sim Gamma(k, \lambda)$$

From the formula for $p_{S_k}$ we can easily get:

$$F_k(T) = \int_0^T p_{S_k}(t) \mathrm{d}t = T \cdot p_{S_k}(T)$$ 

$$F_k(T) \propto (\lambda T)^k e^{-\lambda T}$$

Which in this case is actually telling us that:

$$N(T) \sim Poisson(\lambda T)$$

Et voilà! Going through this convoluted (pun intended) route, we eventually arrived 
at the good old familiar Poisson distribution probability mass function.

### Takeaway
We derived the Poisson distribution as the count of events of a random process from nothing but the following assumptions:
* The events are i.i.d.
* The waiting time between the events is a memoryless continuous random distribution.

We can also see how the Poisson distribution is intimately related to the Gamma distribution:
The Poisson gives counts within an interval and the Gamma gives the length of the interval to achieve a given count,
both using a common rate parameter.
This close relation is further emphasised in Bayesian statistics where the Gamma distribution can serve at the conjugate prior for the rate
of the exponential, Poisson or another Gamma distribution.

If the memoryles assumption doesn't seem realistic, it can be worked around using the general method at the cost of higher complexity.

### Learning 
Using both maximum likelihood estimation
and method of moments, 
we can obtain the estimate for $\lambda$ to be the sample (arithmetic) mean:

$$\hat{\lambda}=\bar{\mathbf{n}}$$

for the counts $\mathbf{n}$.

Compare this to the reciprocal value for the exponential distribution.
The duality between the interpretation of the rate parameter for the Poisson and exponential
distributions can be seen as analogical to the duality between frequency and period.
